{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7252da17-4e85-4ac0-85b1-7114f1f00ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Class Definitions\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from torch.nn import functional as F\n",
    "import json\n",
    "\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model\n",
    "from accelerate.utils import set_seed\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field(default=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "    lora_r: int = field(default=128, metadata={\"help\": \"lora rank\"})\n",
    "    lora_dropout: float = field(default=0.05, metadata={\"help\": \"lora dropout\"})\n",
    "    full_precision: bool = field(default=True, metadata={\"help\": \"whether use int4 for the base model\"})\n",
    "    lora_init: bool = field(default=False, metadata={\"help\": \"True: Use zero and gaussian initialization\"})\n",
    "    token: Optional[str] = field(default=None, metadata={\"help\": \"HF token to access private models\"})\n",
    "    adapter_name_or_path: Optional[str] = field(default=None, metadata={\"help\": \"Path to the LoRA adapter\"})\n",
    "    lora_alpha: int = field(default=16, metadata={\"help\": \"LoRA alpha\"})\n",
    "    ckpt_dir: Optional[str] = field(default=None, metadata={\"help\": \"checkpoint dir for inference.\"})\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    model_max_length: int = field(default=512, metadata={\"help\": \"Maximum sequence length.\"})\n",
    "    num_latent: int = field(default=5, metadata={\"help\": \"The number of latent for training or inference.\"})\n",
    "    use_lora: bool = field(default=True, metadata={\"help\": \"Use lora or not.\"})\n",
    "    greedy: bool = field(default=False, metadata={\"help\": \"Greedy decoding during inference.\"})\n",
    "    use_prj: bool = field(default=False, metadata={\"help\": \"Use a prj module after the llm.\"})\n",
    "    prj_dim: int = field(default=2048, metadata={\"help\": \"The hidden dim of the projection module.\"})\n",
    "    prj_dropout: float = field(default=0.0, metadata={\"help\": \"Dropout ratio of the projection module.\"})\n",
    "    prj_no_ln: bool = field(default=False, metadata={\"help\": \"Remove LayerNorm for the projection module.\"})\n",
    "    inf_latent_iterations: int = field(default=1, metadata={\"help\": \"Latent iterations during inference.\"})\n",
    "    inf_num_iterations: int = field(default=5, metadata={\"help\": \"Run multiple times during inference.\"})\n",
    "    remove_eos: bool = field(default=False, metadata={\"help\": \"Do not add <eos> as a delimiter.\"})\n",
    "\n",
    "class CODI(torch.nn.Module):\n",
    "    def __init__(self, model_args, training_args, lora_config):\n",
    "        super().__init__()\n",
    "        self.model_args = model_args\n",
    "        self.training_args = training_args\n",
    "        self.model_name = model_args.model_name_or_path\n",
    "        \n",
    "        # Load the base model\n",
    "        self.codi = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.bfloat16 if training_args.bf16 else torch.float16,\n",
    "            resume_download=True,\n",
    "        )\n",
    "\n",
    "        ori_vocab_size = self.codi.config.vocab_size\n",
    "        \n",
    "        # Define special tokens\n",
    "        self.pad_token_id = ori_vocab_size\n",
    "        self.bot_id = ori_vocab_size + 1\n",
    "        self.eot_id = ori_vocab_size + 2\n",
    "\n",
    "        self.codi.resize_token_embeddings(ori_vocab_size + 3)\n",
    "        self.dim = self.codi.config.hidden_size\n",
    "\n",
    "        # Apply LoRA configuration\n",
    "        if training_args.use_lora:\n",
    "            self.codi = get_peft_model(self.codi, lora_config)\n",
    "\n",
    "        # Optional Projection Layer\n",
    "        if training_args.use_prj:\n",
    "            self.prj = nn.Sequential(\n",
    "                nn.Dropout(training_args.prj_dropout),\n",
    "                nn.Linear(self.dim, training_args.prj_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(training_args.prj_dim, self.dim),\n",
    "            )\n",
    "            if not training_args.prj_no_ln:\n",
    "                self.prj.add_module(\"ln\", nn.LayerNorm(self.dim))\n",
    "            self.prj.to(self.codi.dtype)\n",
    "\n",
    "    def get_embd(self, model, model_name):\n",
    "        # Helper to get the embedding layer from different model architectures\n",
    "        base_model = model.get_base_model() if hasattr(model, \"get_base_model\") else model\n",
    "        if \"llama\" in model_name.lower() or \"mistral\" in model_name.lower():\n",
    "            return base_model.model.embed_tokens\n",
    "        elif \"phi\" in model_name.lower():\n",
    "             return base_model.model.embed_tokens\n",
    "        elif \"gpt2\" in model_name.lower():\n",
    "            return base_model.transformer.wte\n",
    "        else:\n",
    "            raise NotImplementedError(f\"get_embd not implemented for {model_name}\")\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e1d854-50b0-4f26-88b4-fd02cb9ddf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Helper Functions for Loading and Generation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model_and_tokenizer(model_args, training_args):\n",
    "    \"\"\"Loads the model and tokenizer based on the provided arguments.\"\"\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if not model_args.lora_init:\n",
    "        raise ValueError(\"lora_init must be True for this script.\")\n",
    "\n",
    "    task_type = TaskType.CAUSAL_LM\n",
    "    if any(name in model_args.model_name_or_path.lower() for name in [\"llama\", \"mistral\", \"falcon\", \"qwen\", \"phi\"]):\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    "        if \"phi\" in model_args.model_name_or_path.lower():\n",
    "             target_modules.extend([\"dense\", \"fc1\", \"fc2\"])\n",
    "    elif any(name in model_args.model_name_or_path.lower() for name in [\"gpt2\"]):\n",
    "        target_modules = [\"c_attn\", \"c_proj\", 'c_fc']\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type for LoRA: {model_args.model_name_or_path}.\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=task_type,\n",
    "        inference_mode=False,\n",
    "        r=model_args.lora_r,\n",
    "        lora_alpha=model_args.lora_alpha,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules,\n",
    "        init_lora_weights=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Initializing CODI model...\")\n",
    "    model = CODI(model_args, training_args, lora_config)\n",
    "    \n",
    "    try:\n",
    "        # Load fine-tuned adapter weights\n",
    "        ckpt_path = os.path.expanduser(model_args.ckpt_dir)\n",
    "        bin_path = os.path.join(ckpt_path, \"codi.bin\")\n",
    "\n",
    "        if os.path.exists(bin_path):\n",
    "            print(f\"Loading state dict from: {bin_path}\")\n",
    "            state_dict = torch.load(bin_path, map_location=device)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Could not find model weights in {ckpt_path}\")\n",
    "\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"Successfully loaded state dict.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading state dictionary: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"left\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.to(torch.bfloat16 if training_args.bf16 else torch.float16)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"CODI Model and tokenizer loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_cache(model, tokenizer, perturbed_question, training_args):\n",
    "    \"\"\"\n",
    "    Runs the model on the perturbed prompt and caches the hidden state activations\n",
    "    for each layer of latent thoughts 1 through 6.\n",
    "\n",
    "    Args:\n",
    "        model: The pre-trained language model.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        perturbed_question (str): The perturbed input prompt.\n",
    "        training_args: Configuration object.\n",
    "\n",
    "     Returns:\n",
    "        tuple(dict, str): A tuple containing:\n",
    "            - activation_cache (dict): A cache of activations, structured as \n",
    "                                       {thought_idx: {layer_idx: tensor, ...}, ...}.\n",
    "            - final_answer (str): The generated final answer from the model.\n",
    "    \"\"\"\n",
    "    # 1. Initialize the Cache\n",
    "    activation_cache = {i: {} for i in range(1, training_args.inf_latent_iterations + 1)}\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Preprocess the Input\n",
    "    inputs = tokenizer(perturbed_question, return_tensors=\"pt\").to(device)\n",
    "    bot_tensor = torch.tensor([model.bot_id], dtype=torch.long, device=device).expand(inputs.input_ids.size(0), 1)\n",
    "    input_ids = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "    attention_mask = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 3. Initial pass for \"Latent Thought 0\"\n",
    "        # This sets up the initial state for the reasoning process.\n",
    "        outputs = model.codi(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "\n",
    "        if training_args.use_prj:\n",
    "            latent_embd = model.prj(latent_embd)\n",
    "\n",
    "        # 4. Main Caching Loop: Iterate through Latent Thoughts 1-6\n",
    "        for thought_idx in range(1, training_args.inf_latent_iterations + 1):\n",
    "            outputs = model.codi(\n",
    "                inputs_embeds=latent_embd,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            \n",
    "            # --- CACHING LOGIC ---\n",
    "            for layer_idx, layer_hidden_state in enumerate(outputs.hidden_states):\n",
    "                # Shape is (1, 1, 2048). Clone to store a clean copy.\n",
    "                activation_cache[thought_idx][layer_idx] = layer_hidden_state.clone().detach()\n",
    "            \n",
    "            # Prepare for the next iteration\n",
    "            past_key_values = outputs.past_key_values\n",
    "            latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "\n",
    "            if training_args.use_prj:\n",
    "                latent_embd = model.prj(latent_embd)\n",
    "        \n",
    "        # --- END OF THOUGHT GENERATION AND CACHING ---\n",
    "        # The 'past_key_values' now contains the state after all thoughts.\n",
    "        # We proceed to generate the final answer.\n",
    "\n",
    "        # 5. Generate the Final Answer from the Latent Thoughts\n",
    "        eot_token_id = torch.tensor([model.eot_id], dtype=torch.long, device=device)\n",
    "        next_input_embeds = model.get_embd(model.codi, model.model_name)(eot_token_id).unsqueeze(0)\n",
    "        \n",
    "        generated_token_ids = []\n",
    "        \n",
    "        for _ in range(training_args.model_max_length):\n",
    "            # We no longer need hidden states for the final answer generation\n",
    "            out = model.codi(\n",
    "                inputs_embeds=next_input_embeds,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values,\n",
    "                output_hidden_states=False \n",
    "            )\n",
    "            \n",
    "            past_key_values = out.past_key_values\n",
    "            current_logits = out.logits[:, -1, :]\n",
    "            next_token_id = torch.argmax(current_logits, dim=-1)\n",
    "            \n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            generated_token_ids.append(next_token_id.item())\n",
    "            next_input_embeds = model.get_embd(model.codi, model.model_name)(next_token_id).unsqueeze(0)\n",
    "\n",
    "    # 6. Decode the generated tokens into the final string\n",
    "    final_answer = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # 7. Return both the cache and the final answer\n",
    "    return activation_cache, final_answer\n",
    "\n",
    "def analyze_cache(activation_cache):\n",
    "    \"\"\"\n",
    "    Analyzes and prints the structure and tensor shapes of an activation cache.\n",
    "    \"\"\"\n",
    "    if not activation_cache:\n",
    "        print(\"The activation cache is empty.\")\n",
    "        return\n",
    "\n",
    "    print(\"--- Analyzing Activation Cache Structure ---\")\n",
    "    \n",
    "    # Get the list of thought indices (e.g., [1, 2, 3, 4, 5, 6])\n",
    "    thought_indices = sorted(activation_cache.keys())\n",
    "    print(f\"Cached Thought Indices: {thought_indices}\")\n",
    "    print(f\"Total Thoughts Cached: {len(thought_indices)}\\n\")\n",
    "\n",
    "    # Inspect the first thought in detail to show the structure for all\n",
    "    first_thought_idx = thought_indices[0]\n",
    "    first_thought_layers = activation_cache[first_thought_idx]\n",
    "    \n",
    "    layer_indices = sorted(first_thought_layers.keys())\n",
    "    \n",
    "    print(f\"--- Details for Thought Index: {first_thought_idx} ---\")\n",
    "    print(f\"Cached Layer Indices: {layer_indices[0]} to {layer_indices[-1]}\")\n",
    "    print(f\"Total Layers Cached per Thought: {len(layer_indices)}\")\n",
    "    \n",
    "    # Get the shape of the activation tensor from the first layer of the first thought\n",
    "    # This shape will be consistent across all layers and thoughts.\n",
    "    sample_tensor = first_thought_layers[layer_indices[0]]\n",
    "    tensor_shape = sample_tensor.shape\n",
    "    \n",
    "    print(f\"\\nShape of each activation tensor: {tensor_shape}\")\n",
    "    print(f\"  - Batch Size: {tensor_shape[0]}\")\n",
    "    print(f\"  - Sequence Length: {tensor_shape[1]} (Represents a single thought token)\")\n",
    "    print(f\"  - Hidden Dimension: {tensor_shape[2]}\")\n",
    "\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(\"The cache is a dictionary where:\")\n",
    "    print(\"  - Keys are thought indices (integers from 1 to 6).\")\n",
    "    print(\"  - Values are another dictionary for layers.\")\n",
    "    print(\"      - Keys are layer indices (integers from 0 to N).\")\n",
    "    print(f\"      - Values are PyTorch tensors of shape {tensor_shape}.\")\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Helper context manager for cleaner hook management\n",
    "@contextmanager\n",
    "def apply_forward_hook(module, hook_fn):\n",
    "    \"\"\"Context manager to apply and automatically remove a forward hook.\"\"\"\n",
    "    handle = module.register_forward_hook(hook_fn)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "def run_patch(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    training_args,\n",
    "    original_question,\n",
    "    activation_cache,\n",
    "    thought_idx_to_patch,\n",
    "    layer_idx_to_patch\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs inference on the original prompt while patching a specific activation\n",
    "    from the cache at a designated thought and layer.\n",
    "\n",
    "    Args:\n",
    "        model: The pre-trained language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        training_args: Configuration object.\n",
    "        original_question (str): The clean, unperturbed question.\n",
    "        activation_cache (dict): The cache from Phase 1.\n",
    "        thought_idx_to_patch (int): The latent thought step to patch (1-6).\n",
    "        layer_idx_to_patch (int): The layer index to patch (0 to N-1).\n",
    "\n",
    "    Returns:\n",
    "        str: The final generated answer string from the patched run.\n",
    "    \"\"\"\n",
    "    # 1. Retrieve the specific activation to patch from the cache\n",
    "    # This is the \"corrupted\" state we will inject.\n",
    "    cached_activation = activation_cache[thought_idx_to_patch][layer_idx_to_patch]\n",
    "\n",
    "    # --- Hook Definition and Logic ---\n",
    "    # We use a mutable object (a list) as a counter to track the thought index.\n",
    "    # This allows the inner hook function to modify a variable from the outer scope.\n",
    "    thought_counter = [0] \n",
    "\n",
    "    def patch_activation_hook(module, args, output):\n",
    "        # This hook fires every time a forward pass happens on the hooked layer.\n",
    "        # We only want to patch when we are processing the target thought.\n",
    "        \n",
    "        # The first pass is for the prompt + [Begin_Thought], which we count as thought 0.\n",
    "        # The subsequent passes are for thoughts 1, 2, 3, ...\n",
    "        if thought_counter[0] == thought_idx_to_patch:\n",
    "            # The output of a layer is a tuple in some models (e.g., (hidden_state, ...))\n",
    "            # Or just the tensor itself. Let's assume it's the tensor for simplicity.\n",
    "            # We must handle both cases.\n",
    "            if isinstance(output, tuple):\n",
    "                # Modify the first element, which is typically the hidden state.\n",
    "                # Shape is (batch, seq_len, hidden_dim) -> (1, 1, 2048)\n",
    "                output_list = list(output)\n",
    "                output_list[0] = cached_activation\n",
    "                return tuple(output_list)\n",
    "            else:\n",
    "                # If output is just a tensor, replace it directly.\n",
    "                return cached_activation\n",
    "        \n",
    "        # For all other thoughts, do nothing and return the original output.\n",
    "        return output\n",
    "    \n",
    "    # --- Start of Modified Generation Logic ---\n",
    "    model.eval()\n",
    "\n",
    "    # Get the target layer module. The path may need adjustment.\n",
    "    # Example for a standard Hugging Face GPT-2/Llama architecture:\n",
    "    # `model.codi.model.layers[layer_idx_to_patch]`\n",
    "    # We will assume a path like this for the example.\n",
    "    target_layer = model.codi.base_model.model.model.layers[layer_idx_to_patch]\n",
    "\n",
    "    with torch.no_grad(), apply_forward_hook(target_layer, patch_activation_hook):\n",
    "        # 2. Preprocess the ORIGINAL input\n",
    "        inputs = tokenizer(original_question, return_tensors=\"pt\").to(device)\n",
    "        bot_tensor = torch.tensor([model.bot_id], dtype=torch.long, device=device).expand(inputs.input_ids.size(0), 1)\n",
    "        input_ids = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "        attention_mask = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "        # 3. Run \"Latent Thought 0\" Generation\n",
    "        # The hook will fire here, but since thought_counter[0] is 0, it won't patch\n",
    "        # unless thought_idx_to_patch is also 0 (which we have excluded).\n",
    "        outputs = model.codi(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        thought_counter[0] += 1 # Increment after processing thought 0\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "        if training_args.use_prj:\n",
    "            latent_embd = model.prj(latent_embd)\n",
    "\n",
    "        # 4. Iterate through latent thoughts\n",
    "        for i in range(1, training_args.inf_latent_iterations + 1):\n",
    "            # The hook will fire on the forward pass inside this loop.\n",
    "            # If `thought_counter[0]` matches `thought_idx_to_patch`, the swap happens.\n",
    "            outputs = model.codi(\n",
    "                inputs_embeds=latent_embd,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            thought_counter[0] += 1 # Increment after processing each thought\n",
    "            past_key_values = outputs.past_key_values\n",
    "            latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "            if training_args.use_prj:\n",
    "                latent_embd = model.prj(latent_embd)\n",
    "\n",
    "        # 5. Generate the final answer (same as original `generate` function)\n",
    "        eot_token_id = torch.tensor([model.eot_id], dtype=torch.long, device=device)\n",
    "        next_input_embeds = model.get_embd(model.codi, model.model_name)(eot_token_id).unsqueeze(0)\n",
    "        generated_token_ids = []\n",
    "        for _ in range(training_args.model_max_length):\n",
    "            # The hook will continue to fire here but the counter is now too high, so it won't patch.\n",
    "            out = model.codi(\n",
    "                inputs_embeds=next_input_embeds,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            past_key_values = out.past_key_values\n",
    "            current_logits = out.logits[:, -1, :]\n",
    "            next_token_id = torch.argmax(current_logits, dim=-1)\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            generated_token_ids.append(next_token_id.item())\n",
    "            next_input_embeds = model.get_embd(model.codi, model.model_name)(next_token_id).unsqueeze(0)\n",
    "\n",
    "    # 6. Decode and return the final answer\n",
    "    final_answer = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "    return final_answer\n",
    "\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7ea221-3df5-4981-9fb8-8ef6bc7721c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing CODI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state dict from: ./llama/codi.bin\n",
      "Successfully loaded state dict.\n",
      "Loading tokenizer...\n",
      "CODI Model and tokenizer loaded successfully.\n",
      "\n",
      "All models ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration and Interactive Session\n",
    "# --- Configuration is now set directly in the script ---\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"./llama\", # Load base model from local folder\n",
    "    lora_r=128,\n",
    "    lora_alpha=32,\n",
    "    lora_init=True,\n",
    "    ckpt_dir=\"./llama\" # Path to fine-tuned adapter\n",
    ")\n",
    "\n",
    "# Note: TrainingArguments is used for model config, not actual training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\", # Dummy output dir\n",
    "    seed=11,\n",
    "    model_max_length=512,\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    greedy=True,\n",
    "    num_latent=6,\n",
    "    use_prj=True,\n",
    "    prj_dim=2048,\n",
    "    prj_no_ln=False,\n",
    "    prj_dropout=0.0,\n",
    "    inf_latent_iterations=6,\n",
    "    remove_eos=True,\n",
    "    use_lora=True,\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "codi_model, tokenizer = load_model_and_tokenizer(model_args, training_args)\n",
    "\n",
    "if codi_model and tokenizer:\n",
    "    print(\"\\nAll models ready!\")\n",
    "else:\n",
    "    print(\"ERROR loading models, read above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9fa288d-4c24-49de-824a-7c0023c34f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Interactive Mode ---\n",
      "Enter your question below. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Question:  Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      " Modified Question:  Janet’s ducks lay 11 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating with CODI model...\n",
      "Cache complete\n",
      "\n",
      "--- Modified Answer ---\n",
      "The answer is: 8\n",
      "--------------------\n",
      "\n",
      "--- Patched Answer ---\n",
      "The answer is: 18\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Original Question:  exit\n"
     ]
    }
   ],
   "source": [
    "#Cell 4: Run Loop\n",
    "if codi_model and tokenizer:\n",
    "    print(\"\\n--- Interactive Mode ---\")\n",
    "    print(\"Enter your question below. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            user_question = input(\"\\n Original Question: \")\n",
    "            if user_question.lower() == 'exit':\n",
    "                break\n",
    "            modif = input(\"\\n Modified Question: \")\n",
    "            print(\"\\nGenerating with CODI model...\")\n",
    "            cache, final_answer = generate_cache(codi_model, tokenizer, modif, training_args)\n",
    "            print(\"Cache complete\")\n",
    "            #original question, cache, thought (1-6), layer (0-15)\n",
    "            patched = run_patch(codi_model, tokenizer, training_args, user_question, cache, 1, 5)\n",
    "            # analyze_cache(cache)exit\n",
    "            \n",
    "            print(\"\\n--- Modified Answer ---\")\n",
    "            print(final_answer.strip())\n",
    "            print(\"--------------------\")\n",
    "\n",
    "            print(\"\\n--- Patched Answer ---\")\n",
    "            print(patched.strip())\n",
    "            print(\"--------------------\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58d95c-a1bb-4c46-8185-640fe09ec40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
